# üöÄ GPT-5 Upgrade Guide

**Project Veritas now uses GPT-5!** Released August 2025.

---

## üéØ Why GPT-5 for Fake Review Detection?

### Performance Improvements Over GPT-4o

| Metric | GPT-4o-mini | GPT-5-mini | Improvement |
|--------|-------------|------------|-------------|
| **SWE-bench Score** | ~40% | 74.9% | +87% better |
| **Hallucinations** | Baseline | 45% fewer | Much more reliable |
| **Math Reasoning (AIME)** | ~70% | 94.6% | +35% better |
| **Context Window** | 128K tokens | 272K tokens | 2x larger |

### Why This Matters for Reviews:

‚úÖ **45% Fewer Hallucinations** = More accurate fake detection
‚úÖ **Superior Reasoning** = Better at detecting subtle deception
‚úÖ **Adaptive Intelligence** = Automatically scales complexity
‚úÖ **Lower Cost** = $1.25/1M input tokens (50% cheaper than GPT-4o input)

---

## üìä GPT-5 Model Options

Project Veritas is configured to use **GPT-5-mini** by default (best balance).

### Available Models:

| Model | Best For | Cost Level | Use Case |
|-------|----------|------------|----------|
| **gpt-5-mini** | Most users | Medium | ‚≠ê **RECOMMENDED** - Best accuracy/cost balance |
| **gpt-5** | Premium | High | Maximum accuracy, professional use |
| **gpt-5-nano** | Budget | Low | Cost-sensitive applications |

### How to Change Model:

Edit [config.py](config.py):

```python
# Option 1: Best balance (default)
OPENAI_MODEL = "gpt-5-mini"

# Option 2: Maximum accuracy (higher cost)
OPENAI_MODEL = "gpt-5"

# Option 3: Budget option (lower accuracy)
OPENAI_MODEL = "gpt-5-nano"
```

---

## üí∞ Pricing Comparison

### Input Tokens (per 1M):

- **GPT-4o**: $2.50
- **GPT-5**: $1.25 ‚úÖ **50% cheaper**
- **GPT-5-mini**: ~$0.50-1.00 (estimated)
- **GPT-5-nano**: ~$0.15-0.30 (estimated)

### Output Tokens (per 1M):

- **GPT-4o**: $10.00
- **GPT-5**: $10.00 (same)

### Typical Project Veritas Analysis:

**Per Analysis (with AI enabled)**:
- Input: ~2,000 tokens (reviewing 10 sample reviews)
- Output: ~500 tokens (analysis results)
- **Cost with GPT-5-mini**: ~$0.001-0.002 per analysis
- **100 analyses**: ~$0.10-0.20/month üéâ

**Even cheaper than before!**

---

## üîß What Changed in Project Veritas

### Files Updated:

1. **[config.py](config.py)**
   - Changed default model from `gpt-4o-mini` to `gpt-5-mini`
   - Added detailed model comparison comments
   - Documented pricing and performance benefits

2. **[src/ai_analyzer.py](src/ai_analyzer.py)**
   - Updated class docstrings to reference GPT-5
   - Added GPT-5 benefits in module header
   - Model fallback now defaults to `gpt-5-mini`

3. **Documentation**
   - Updated pricing estimates
   - Added this upgrade guide
   - Mentioned GPT-5 in README features

### What Stayed the Same:

‚úÖ **All functionality** - No breaking changes
‚úÖ **API calls** - Same OpenAI API structure
‚úÖ **Cost estimates** - Actually CHEAPER now!
‚úÖ **Setup process** - Same OPENAI_API_KEY configuration

---

## üöÄ How to Upgrade

### If You Already Installed:

**No action needed!** Just update your code:

```bash
cd "/Users/apexacceleration/My Drive (tyler@apexacceleration.com) (1)/Software Projects/Project Veritas"
git pull  # If using git
```

The config file now defaults to `gpt-5-mini` automatically.

### If Starting Fresh:

Follow the normal installation in [QUICKSTART.md](QUICKSTART.md):

```bash
pip install -r requirements.txt
cp .env.example .env
# Add OPENAI_API_KEY to .env
streamlit run app.py
```

---

## üìà Expected Improvements

### Fake Review Detection Accuracy:

Based on GPT-5's improvements:

1. **Fewer False Positives** (45% fewer hallucinations)
   - More reliable "suspicious" flags
   - Better at distinguishing authentic detailed reviews from fake ones

2. **Better Linguistic Analysis**
   - Superior at detecting subtle deception patterns
   - More nuanced understanding of review context
   - Better sentiment-rating coherence analysis

3. **More Reliable Batch Analysis**
   - Better at identifying coordinated campaigns
   - Improved pattern recognition across multiple reviews

### Trust Score Impact:

You may see:
- ‚úÖ **More accurate Trust Scores** (especially in edge cases)
- ‚úÖ **More detailed AI reasoning** in reports
- ‚úÖ **Fewer "unknown" manipulation likelihood ratings**

---

## üß™ Testing GPT-5 vs GPT-4o

Want to compare? Test both:

### Edit config.py:

```python
# Test GPT-4o-mini (old)
OPENAI_MODEL = "gpt-4o-mini"
```

Run analysis, save report:
```bash
python src/main.py AMAZON_URL -o report_gpt4o.json
```

### Switch to GPT-5-mini:

```python
# Test GPT-5-mini (new)
OPENAI_MODEL = "gpt-5-mini"
```

Run again:
```bash
python src/main.py SAME_AMAZON_URL -o report_gpt5.json
```

### Compare Results:

- Check Trust Scores
- Read AI insights reasoning
- Compare red flags triggered
- Note any differences in suspicious review detection

Share your findings! GPT-5 should be more accurate and reliable.

---

## üîê Security & Rate Limits

### API Key:

Same as before - set in `.env`:
```
OPENAI_API_KEY=sk-your-key-here
```

### Rate Limits:

GPT-5 has the same rate limit structure as GPT-4:
- **Tier 1** (new accounts): 500 RPM, 30K TPM
- **Tier 2**: 5K RPM, 450K TPM
- **Tier 3+**: Higher limits

For typical Project Veritas use (1-2 analyses at a time), Tier 1 is sufficient.

---

## üåê Deployment Considerations

### Streamlit Cloud:

No changes needed! Same setup:

1. Add `OPENAI_API_KEY` to Streamlit secrets
2. Deploy as normal
3. App will automatically use `gpt-5-mini` from config

### Cost Monitoring:

Monitor usage at: [platform.openai.com/usage](https://platform.openai.com/usage)

Set usage limits:
1. Go to [platform.openai.com/settings/organization/limits](https://platform.openai.com/settings/organization/limits)
2. Set monthly limit (e.g., $5/month for safety)

---

## ‚ùì FAQ

**Q: Do I need a new API key?**
A: No, your existing OpenAI API key works with GPT-5.

**Q: Will this cost more?**
A: No! GPT-5 input tokens are 50% cheaper than GPT-4o.

**Q: Can I still use GPT-4o?**
A: Yes, just change `OPENAI_MODEL` in config.py to `"gpt-4o-mini"`.

**Q: Is GPT-5 available in my region?**
A: GPT-5 is available in all regions where OpenAI API is accessible.

**Q: What if GPT-5 isn't available yet?**
A: The code falls back gracefully. Set `OPENAI_MODEL = "gpt-4o-mini"` if needed.

**Q: Should I use gpt-5, gpt-5-mini, or gpt-5-nano?**
A: **gpt-5-mini** is recommended for best balance. Use `gpt-5` only if you need maximum accuracy and have budget.

---

## üéâ Summary

‚úÖ **Upgraded to GPT-5-mini** (better accuracy, lower cost)
‚úÖ **No breaking changes** (drop-in replacement)
‚úÖ **~50% cheaper** on input tokens
‚úÖ **45% fewer hallucinations** (more reliable)
‚úÖ **Superior reasoning** (better fake detection)
‚úÖ **Same setup process** (no new configuration)

**You're now running the most advanced fake review detection AI available!** üöÄ

---

**Questions?** Check:
- [QUICKSTART.md](QUICKSTART.md) - Setup guide
- [README.md](README.md) - Full docs
- [config.py](config.py) - Model configuration

**Want to discuss GPT-5 performance?** Open a GitHub issue with your findings!
